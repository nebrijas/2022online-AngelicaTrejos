{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc994ed",
   "metadata": {},
   "source": [
    "# Ejercicio con Python para lograr un scraping de una web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f7e7c",
   "metadata": {},
   "source": [
    "# Iniciamos con el ejercicio de programación literaria, lo primero es importar las librerías y módulos necesarios para el desarrollo de la actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493cd14",
   "metadata": {},
   "source": [
    "## Código fuente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fdba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "resultados = []\n",
    "\n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "tags = soup.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3533c",
   "metadata": {},
   "source": [
    "### luego, instalamos las librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4417e589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\angelica\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\angelica\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\angelica\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=71de9df0a3e0e1c2a21afa98a5a15644f355608dd12a556eab53e6a4df91015c\n",
      "  Stored in directory: c:\\users\\angelica\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: termcolor, bs4\n",
      "Successfully installed bs4-0.0.1 termcolor-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0356e9e",
   "metadata": {},
   "source": [
    "### Ahora importamos las librerias internas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c619b",
   "metadata": {},
   "source": [
    "Con time podemos utilizar sus funciones manipular la fecha y hora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209a445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "207eca92",
   "metadata": {},
   "source": [
    "posteriormente, usamos el formato para importar y exportar hojas de cálculo y bases de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c7f23",
   "metadata": {},
   "source": [
    "seguidamente, el módulo que proporciona operaciones de coincidencia de expresiones regulares similares o en cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ac656",
   "metadata": {},
   "source": [
    "avanzamos con la función que permite ver en python las funcionalidades del sistema operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750188c",
   "metadata": {},
   "outputs": [],
   "source": [
    " import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1102e",
   "metadata": {},
   "source": [
    "### A continuación procedemos a importar las librerías externas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce1498",
   "metadata": {},
   "source": [
    "iniciamos con termcolor, que sirve para imprimir mensajes de colores, es decir, el texto coloreado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "luego, avanzamos con la opción requests, que es una especie de biblioteca de python trabajar con HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744669b",
   "metadata": {},
   "source": [
    "insertamos bs4, que es librería que facilita extraer información de páginas web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4dd000",
   "metadata": {},
   "source": [
    "seguidamente \"pandas\", paquete de herramientas de análisis de datos de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fa146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7b375",
   "metadata": {},
   "source": [
    "## Es momento de poner los *objetos/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8d6cb",
   "metadata": {},
   "source": [
    "Se crea un objeto vacío denominado resultados en el que se añadirán los resultados del scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f1331",
   "metadata": {},
   "source": [
    "si pasamos a la función \"type\" permite observar que es un objetivo de tipo lista de python, lo que es básico y fundamental para organizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2089496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823e80a",
   "metadata": {},
   "source": [
    "### solicitud de acceso a los datos de un sitio web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c30bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Se agrega una variable para utilizar la solicitud req=requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65172083",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86d874",
   "metadata": {},
   "source": [
    "Después escribimos la función type() y esa es la respuesta de request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d6eade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880cc20",
   "metadata": {},
   "source": [
    "Se utiliza el condicional if para que cuando el status no sea 200 el valor de regreso no pueda leerse. el valor 200 significa que la solicitud fue exitosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d2c3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si el estatus no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    "    raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9997b5",
   "metadata": {},
   "source": [
    "### Beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa54d4",
   "metadata": {},
   "source": [
    "Agregamos otra variable para aplicar el Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbcd2c",
   "metadata": {},
   "source": [
    "Ademas agregamos la variable que funciona para encontrar todas las etiquetas \"h2\" dentro de la página anteriormente especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b90a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.findAll(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26e976",
   "metadata": {},
   "source": [
    "Se indica a través de un for, que se encuentren todos los h2 dentro de la variable anterior y se imprima el texto dentro de la etiqueta. Luego con resultados.append(h2.text) se añaden todos los textos dentro de las etiquetas a la lista inicial resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a446fb",
   "metadata": {},
   "source": [
    "### Todo el proceso se repite en las diferentes categorías o secciones del medio de comunicación El País, es decir, el mismo código pero cambia según la sección *(Educación, económicas, internacional, deportes, opinión, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f9671d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las protestas se extienden en China contra la política de covid cero\n",
      "Hartazgo, estragos económicos y accidentes mortales: claves para entender las protestas en China\n",
      "El último Iphone, perfumes de Chanel o un Rolex: las marcas occidentales llenan los escaparates rusos pese a las sanciones\n",
      "Publicar no es un delito\n",
      "La vida bajo los bombardeos rusos\n",
      "Democracias a la defensiva\n",
      "La UE vista desde el G-20 de Bali \n",
      "El arte ruso de la provocación\n",
      "China y Estados Unidos, condenadas a entenderse y a rivalizar\n",
      "Lecciones en Ucrania para ser reportero de guerra: explosiones de fogueo, torniquetes y campos de minas\n",
      "De la manifestación al patíbulo: el régimen iraní amenaza con la horca para sofocar las protestas\n",
      "Publicar no es un delito\n",
      "Los medios de los cables de WikiLeaks piden a EE UU que no persiga a Assange\n",
      "La gestión del frío enfrenta a Zelenski y al alcalde de Kiev\n",
      "El sueño hecho realidad del refugiado sirio del selfi con Merkel\n",
      "Los trabajadores migrantes de provincias en China estallan contra los confinamientos por covid\n",
      "Sobrevivir bajo cero en Ucrania\n",
      "La UE teme que los ataques rusos contra la red eléctrica de Ucrania fuercen otra oleada de refugiados\n",
      "Encuentran el cadáver de una niña desaparecida en el corrimiento de tierra en la isla italiana de Ischia\n",
      "Justin Trudeau justifica el uso de la Ley de emergencia para desalojar a los camioneros antivacunas\n",
      "La nota del autor de la matanza del Walmart de Virginia: “Se reían de mí y decían que era un asesino en serie”\n",
      "La pobreza se ceba con los menores de 18 años en América Latina\n",
      "Tres horas libres en el trabajo para ver el partido… ¡Bienvenidos a Brasil!\n",
      "Petro y López Obrador estrenan un eje latinoamericano con el éxito de la negociación de Venezuela como trasfondo\n"
     ]
    }
   ],
   "source": [
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909283bb",
   "metadata": {},
   "source": [
    "### En este apartado dedicamos espacio para los titulares con las palabras más buscadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ab634",
   "metadata": {},
   "source": [
    "En esta parte usaremos os.system(clear) para se haga una limpieza después de realizar el web scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e84fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e03f5",
   "metadata": {},
   "source": [
    "A continuación se mostrarán los títulares en negrita (stilo) con la siguiente función"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c373a5",
   "metadata": {},
   "source": [
    "De la misma manera se utilizará la función \"for\" para verificar que contenga la palabra clave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ba0ac",
   "metadata": {},
   "source": [
    "Ahora se repite el mismo procedimiento con las demás palabras clave en el resto de las secciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
